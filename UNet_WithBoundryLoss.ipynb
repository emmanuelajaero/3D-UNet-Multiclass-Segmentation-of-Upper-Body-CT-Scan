{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83d95e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94e18ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONAI version: 1.1.0\n",
      "Numpy version: 1.21.5\n",
      "Pytorch version: 1.13.1+cu117\n",
      "MONAI flags: HAS_EXT = False, USE_COMPILED = False, USE_META_DICT = False\n",
      "MONAI rev id: a2ec3752f54bfc3b40e7952234fbeb5452ed63e3\n",
      "MONAI __file__: /home/emmanuel/PycharmProjects/pythonProject/venv/lib/python3.7/site-packages/monai/__init__.py\n",
      "\n",
      "Optional dependencies:\n",
      "Pytorch Ignite version: 0.4.11\n",
      "Nibabel version: 4.0.2\n",
      "scikit-image version: 0.19.3\n",
      "Pillow version: 9.5.0\n",
      "Tensorboard version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "gdown version: 4.7.1\n",
      "TorchVision version: 0.14.1+cu117\n",
      "tqdm version: 4.65.0\n",
      "lmdb version: 1.4.1\n",
      "psutil version: 5.9.5\n",
      "pandas version: 1.3.5\n",
      "einops version: 0.6.1\n",
      "transformers version: 4.28.1\n",
      "mlflow version: 1.30.1\n",
      "pynrrd version: 1.0.0\n",
      "\n",
      "For details about installing the optional dependencies, please visit:\n",
      "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import concurrent.futures\n",
    "\n",
    "from monai.data.utils import pad_list_data_collate\n",
    "\n",
    "import monai.losses as losses\n",
    "import matplotlib.pyplot as plt\n",
    "from monai.networks.layers import Norm\n",
    "\n",
    "from monai.losses import DiceCELoss\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.transforms import Transform\n",
    "from monai.transforms import (\n",
    "    AsDiscrete,\n",
    "    Compose,\n",
    "    SpatialPad,\n",
    "    SpatialPadd,\n",
    "    CropForegroundd,\n",
    "    LoadImaged,\n",
    "    Orientationd,\n",
    "    RandFlipd,\n",
    "    RandCropByPosNegLabeld,\n",
    "    RandCropByLabelClassesd,\n",
    "    RandShiftIntensityd,\n",
    "    ScaleIntensityRanged,\n",
    "    NormalizeIntensity,\n",
    "    NormalizeIntensityd,\n",
    "    Spacingd,\n",
    "    RandRotate90d,\n",
    "    EnsureTyped,\n",
    "    RandGaussianNoised,\n",
    ")\n",
    "\n",
    "\n",
    "from monai.config import print_config\n",
    "from monai.metrics import DiceMetric, HausdorffDistanceMetric\n",
    "from monai.networks.nets import UNet\n",
    "\n",
    "from monai.data import (\n",
    "    DataLoader,\n",
    "    ThreadDataLoader,\n",
    "    SmartCacheDataset,\n",
    "    PersistentDataset,\n",
    "    load_decathlon_datalist,\n",
    "    decollate_batch,\n",
    "    set_track_meta,\n",
    ")\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CyclicLR\n",
    "\n",
    "from boundary_loss import BDLoss, DC_and_BD_loss\n",
    "\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c27ce420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model/best_metric_model_with_boundry_loss.pth\n"
     ]
    }
   ],
   "source": [
    "directory = os.environ.get(\"MONAI_DATA_DIRECTORY\")\n",
    "root_dir = 'Model'\n",
    "model_name =  os.path.join(root_dir, \"best_metric_model_with_boundry_loss.pth\")\n",
    "print(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ced24050",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombineBinaryMaps(Transform):\n",
    "    def __init__(self, num_classes, keys):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def __call__(self, data):\n",
    "        binary_maps = data[\"label\"]\n",
    "        combined_map = torch.zeros_like(data['image'][0])\n",
    "\n",
    "        for i in range(self.num_classes):\n",
    "            zero_indices = np.where(combined_map == 0)\n",
    "            combined_map[zero_indices] += (i + 1) * binary_maps[i][zero_indices]\n",
    "#             combined_map %= (self.num_classes + 1)\n",
    "\n",
    "        data[\"label\"] = combined_map.long().unsqueeze(0)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3ab7389",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCustomDataReader(Transform):\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "\n",
    "    def __call__(self, img):\n",
    "        # Load data from the data_dir\n",
    "        file_path = os.path.join(self.data_dir, img)\n",
    "        loaded_data = np.load(file_path)  # Customize this to your specific data format\n",
    "\n",
    "        return loaded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "891f3c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 4\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ToDo: Try windowing with W:350 and L:40\n",
    "# define the window width and level\n",
    "window_width = 350\n",
    "window_level = 40\n",
    "\n",
    "# calculate the intensity range to clip\n",
    "intensity_min = window_level - window_width / 2.0\n",
    "intensity_max = window_level + window_width / 2.0\n",
    "\n",
    "train_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\", \"label\"], ensure_channel_first=True, image_only=False),\n",
    "        CombineBinaryMaps(keys=[\"label\"], num_classes=10),\n",
    "#         RandGaussianNoised(keys=[\"image\"], prob=0.50, mean=0.0, std=0.1),\n",
    "\n",
    "        ScaleIntensityRanged(\n",
    "            keys=[\"image\"],\n",
    "            a_min=-175,\n",
    "            a_max=250,\n",
    "            b_min=0.0,\n",
    "            b_max=1.0,\n",
    "            clip=True,\n",
    "        ),\n",
    "        CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
    "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "        Spacingd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            pixdim=(1.5, 1.5, 2.0),\n",
    "            mode=(\"bilinear\", \"nearest\"),\n",
    "        ),\n",
    "        SpatialPadd(\n",
    "            spatial_size=(96, 96, 96),\n",
    "            keys=[\"image\", \"label\"]\n",
    "        ),\n",
    "        EnsureTyped(keys=[\"image\", \"label\"], device=device, track_meta=False),\n",
    "       \n",
    "#         RandCropByLabelClassesd(\n",
    "#             keys=[\"image\", \"label\"],\n",
    "#             label_key=\"label\",\n",
    "#             spatial_size=(96, 96, 96),\n",
    "#             num_samples=num_samples,\n",
    "#             num_classes=11,\n",
    "#             image_key=\"image\",\n",
    "#             image_threshold=0,\n",
    "#             allow_smaller=True\n",
    "#         ),\n",
    "        RandCropByPosNegLabeld(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            label_key=\"label\",\n",
    "            spatial_size=(96, 96, 96),\n",
    "            pos=1,\n",
    "            neg=1,\n",
    "            num_samples=num_samples,\n",
    "            image_key=\"image\",\n",
    "            image_threshold=0,\n",
    "            allow_smaller=True\n",
    "        ),\n",
    "\n",
    "        RandFlipd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            spatial_axis=[0],\n",
    "            prob=0.10,\n",
    "        ),\n",
    "        RandFlipd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            spatial_axis=[1],\n",
    "            prob=0.10,\n",
    "        ),\n",
    "        RandFlipd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            spatial_axis=[2],\n",
    "            prob=0.10,\n",
    "        ),\n",
    "        RandRotate90d(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            prob=0.10,\n",
    "            max_k=3,\n",
    "        ),\n",
    "        RandShiftIntensityd(\n",
    "            keys=[\"image\"],\n",
    "            offsets=0.10,\n",
    "            prob=0.50,\n",
    "        ),\n",
    "\n",
    "    ]\n",
    ")\n",
    "val_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\", \"label\"], ensure_channel_first=True, image_only=False),\n",
    "        CombineBinaryMaps(keys=[\"label\"], num_classes=10),\n",
    "        ScaleIntensityRanged(keys=[\"image\"], a_min=-175, a_max=250, b_min=0.0, b_max=1.0, clip=True),\n",
    "        CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
    "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "        Spacingd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            pixdim=(1.5, 1.5, 2.0),\n",
    "            mode=(\"bilinear\", \"nearest\"),\n",
    "        ),\n",
    "        SpatialPadd(\n",
    "            spatial_size=(96, 96, 96),\n",
    "            keys=[\"image\", \"label\"]\n",
    "        ),\n",
    "        EnsureTyped(keys=[\"image\", \"label\"], device=device, track_meta=True),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9797df2",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdad2045",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_file_path(root_path):\n",
    "    all_files = os.listdir(root_path)\n",
    "    return [{'image': f'{root_path}/{i}/ct.nii.gz',\n",
    "             'label': [f'{root_path}/{i}/segments/{f}' for f in os.listdir(f'{root_path}/{i}/segments')],\n",
    "#              'label': f'{root_path}/{i}/labels.nii.gz'\n",
    "\n",
    "            } \n",
    "            for i in all_files if os.path.isfile(f'{root_path}/{i}/ct.nii.gz')]\n",
    "\n",
    "def data_reader(file_addr):\n",
    "    # load the image data\n",
    "    image_read = nib.load(item[\"image\"])\n",
    "    image_data = image_read.get_fdata(dtype=np.float32)\n",
    "    header = image_read.header\n",
    "    # load the label data\n",
    "    label_data = nib.load(item[\"label\"]).get_fdata(dtype=np.float32)\n",
    "    # Extract the metadata from the header\n",
    "    metadata = {'dim': header.get_data_shape(),\n",
    "                'pixdim': header.get_zooms(),\n",
    "                'affine': header.get_qform()}\n",
    "    data = {\n",
    "        'image': image_data,\n",
    "        'label': label_data,\n",
    "        'metadata': metadata,\n",
    "    }\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b35b04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = 'Dataset'\n",
    "file_list_train = generate_file_path(root_path=f'{root}/train')\n",
    "file_list_val = generate_file_path(root_path=f'{root}/val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12093c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    transform = SpatialPad(\n",
    "            (96, 96, 96),\n",
    "        )\n",
    "#     print([item for item in batch[0]])\n",
    "    images = [transform(item[\"image\"]) for item in batch[0]]\n",
    "    labels = [transform(item[\"label\"]) for item in batch[0]]\n",
    "#     print(images.shape, labels.shape)\n",
    "    return {\"image\": torch.stack(images),\n",
    "           \"label\":torch.stack(labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5ba7b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = PersistentDataset(\n",
    "    data=file_list_train,\n",
    "    transform=train_transforms,\n",
    "    cache_dir='train_unet'\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=1, shuffle=True,\n",
    "                                collate_fn=lambda x: pad_list_data_collate(x, pad_to_shape=(96, 96, 96))\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af87b9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds = PersistentDataset(\n",
    "    data=file_list_val,\n",
    "    transform=val_transforms,\n",
    "    cache_dir='val'\n",
    "#     cache_dir='C:/Training/val'\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(val_ds, batch_size=1, \n",
    "                              collate_fn=lambda x: pad_list_data_collate(x, pad_to_shape=(96, 96, 96)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f84088c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7ff890e5",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de0a8a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, tolerance=5, min_delta=0):\n",
    "\n",
    "        self.tolerance = tolerance\n",
    "        self.min_delta = min_delta\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = loss\n",
    "        else:\n",
    "            if (self.best_loss - loss) > self.min_delta:\n",
    "                self.best_loss = loss\n",
    "                self.counter = 0\n",
    "            else:\n",
    "                self.counter += 1\n",
    "                print(f'Early Stopping patience: {self.counter}, best loss: {self.best_loss}, current_loss: {loss}')\n",
    "                if self.counter >= self.tolerance:  \n",
    "                    self.early_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a6c1475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available GPUs: 1\n",
      "GPU 0: GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device_count = torch.cuda.device_count()\n",
    "    print(f\"Number of available GPUs: {device_count}\")\n",
    "    for i in range(device_count):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    print(\"No GPU found, using CPU instead.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6d51d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model = UNet(\n",
    "              spatial_dims=3,\n",
    "              in_channels=1,\n",
    "              out_channels=11,\n",
    "              channels=(16, 32, 64, 128, 256),\n",
    "              strides=(2, 2, 2, 2),\n",
    "              num_res_units=2,\n",
    "              norm=Norm.BATCH\n",
    "                ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "20b75b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "early_stopping = EarlyStopping(tolerance=10, min_delta=0.001)\n",
    "loss_function = DiceCELoss(to_onehot_y=True, softmax=True, lambda_dice=1, lambda_ce=1)\n",
    "bd_loss =  BDLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "scheduler = ReduceLROnPlateau(optimizer, patience=10, verbose=True, mode='min', min_lr=1e-8, factor=0.9)\n",
    "scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c967d45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = SpatialPad(\n",
    "            (1, 96, 96, 96),\n",
    "        )\n",
    "def validation(epoch_iterator_val):\n",
    "    model.eval()\n",
    "    model.to('cpu')\n",
    "    validation_loss = []\n",
    "    with torch.no_grad():\n",
    "        for batch in epoch_iterator_val:\n",
    "            val_inputs, val_labels = (batch[\"image\"].cpu(), batch[\"label\"].cpu())\n",
    "#             with torch.cuda.amp.autocast():\n",
    "            val_outputs = sliding_window_inference(val_inputs, (96, 96, 96), num_samples, model)\n",
    "            val_labels_list = decollate_batch(val_labels)\n",
    "            val_labels_convert = [post_label(val_label_tensor) for val_label_tensor in val_labels_list]\n",
    "            val_outputs_list = decollate_batch(val_outputs)\n",
    "            val_output_convert = [post_pred(val_pred_tensor) for val_pred_tensor in val_outputs_list]\n",
    "            \n",
    "            \n",
    "            loss = loss_function(val_outputs, val_labels).item()\n",
    "            val_output_convert = torch.stack(val_output_convert)\n",
    "            val_labels_convert = torch.stack(val_labels_convert)\n",
    "            boundry_loss = bd_loss(val_output_convert, val_labels_convert)\n",
    "            loss += 0.5 * boundry_loss.item()\n",
    "            \n",
    "            loss = torch.tensor(loss)\n",
    "            \n",
    "            validation_loss.append(loss.item())\n",
    "            dice_metric(y_pred=val_output_convert, y=val_labels_convert)\n",
    "            del val_output_convert, val_labels_convert, val_outputs\n",
    "#             epoch_iterator_val.set_description(f\"Validate (loss={loss.item():2.5f})\")\n",
    "            epoch_iterator_val.set_description(f\"Validate (loss={loss:2.5f}) (boundry_loss={0.5 * boundry_loss.item():2.5f})\")\n",
    "#             break\n",
    "        mean_dice_val = dice_metric.aggregate().item()\n",
    "        dice_metric.reset()\n",
    "#         val_inputs, val_labels = val_inputs.cpu(), val_labels.cpu()\n",
    "        mean_hausdorff = 0\n",
    "\n",
    "        validation_loss_mean = np.nanmean(np.nan_to_num(np.array(validation_loss),\n",
    "                                               nan=np.nan, posinf=np.nan, neginf=np.nan))\n",
    "\n",
    "        val_loss_values.append(validation_loss_mean)\n",
    "    return mean_dice_val, mean_hausdorff, validation_loss_mean\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "def train(global_step, train_loader, dice_val_best, global_step_best):\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    epoch_loss = 0\n",
    "    step = 0\n",
    "    epoch_iterator = tqdm(train_loader, desc=\"Training (X / X Steps) (loss=X.X)\", dynamic_ncols=True)\n",
    "    for step, batch in enumerate(epoch_iterator):\n",
    "        step += 1\n",
    "#         print(step)\n",
    "        x, y = (batch[\"image\"].cuda(), batch[\"label\"].cuda())\n",
    "#         a = all(t.shape == x.shape for t in y)\n",
    "        if x.shape != y.shape:\n",
    "            print(x.shape, y.shape)\n",
    "        with torch.cuda.amp.autocast():\n",
    "            logit_map = model(x)\n",
    "            loss = loss_function(logit_map, y)\n",
    "\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        y_one_hot = [post_label(y_tensor) for y_tensor in y]\n",
    "        y_one_hot = torch.stack(y_one_hot)\n",
    "        boundry_loss = bd_loss(logit_map, y)\n",
    "        epoch_loss += loss.item() + 0.5 * boundry_loss.item()\n",
    "\n",
    "\n",
    "        if torch.isnan(loss):\n",
    "            print(torch.max(x), torch.min(x))\n",
    "            \n",
    "        scaler.unscale_(optimizer)\n",
    "\n",
    "        scaler.step(optimizer)\n",
    "\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # epoch_iterator.set_description(f\"Training ({global_step} / {max_iterations} Steps) (loss={loss.item():2.5f})\")\n",
    "        epoch_iterator.set_description(f\"Training ({global_step} / {max_iterations} Steps) (loss={loss.item() + (0.5 * boundry_loss.item()):2.5f}) (boundry_loss={0.5 * boundry_loss.item():2.5f})\")\n",
    "        global_step += 1\n",
    "        break\n",
    "       \n",
    "\n",
    "    epoch_loss /= step\n",
    "    return global_step, dice_val_best, global_step_best, epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b6f5d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (0 / 90000 Steps) (loss=3.62576):   0%|                                                                                                                                                   | 0/357 [00:02<?, ?it/s]\n",
      "Validate (loss=3.42948) (boundry_loss=0.07518):  17%|██████████████████████▌                                                                                                                | 4/24 [01:36<08:38, 25.91s/it]"
     ]
    }
   ],
   "source": [
    "max_iterations = 90000\n",
    "\n",
    "post_label = AsDiscrete(to_onehot=11)\n",
    "post_pred = AsDiscrete(argmax=True, to_onehot=11)\n",
    "\n",
    "dice_metric = DiceMetric(include_background=True, reduction=\"mean\", get_not_nans=False)\n",
    "hausdorff_metric = HausdorffDistanceMetric(include_background=True, get_not_nans=False, reduction=\"none\",\n",
    "                                           distance_metric=\"euclidean\")\n",
    "# remove this section future run\n",
    "global_step = 0\n",
    "dice_val_best = 0.0\n",
    "global_step_best = 0\n",
    "epoch_loss_values = []\n",
    "metric_values = []\n",
    "val_loss_values = []\n",
    "while global_step < max_iterations:\n",
    "    global_step, dice_val_best, global_step_best, epoch_loss = train(global_step, train_loader, dice_val_best, global_step_best)\n",
    "    epoch_iterator_val = tqdm(val_loader, desc=\"Validate\", dynamic_ncols=True)\n",
    "    dice_val, hausdorff_val, loss_val = validation(epoch_iterator_val)\n",
    "    scheduler.step(epoch_loss)\n",
    "    epoch_loss_values.append(epoch_loss)\n",
    "    metric_values.append(dice_val)\n",
    "    early_stopping(loss_val)\n",
    "    print(f'Mean Hausdorff disatnce: {hausdorff_val}')\n",
    "    if dice_val > dice_val_best:\n",
    "        dice_val_best = dice_val\n",
    "        global_step_best = global_step\n",
    "        torch.save(model.state_dict(), model_name)\n",
    "        print(\n",
    "            \"Model Was Saved ! Current Best Avg. Dice: {} Current Avg. Dice: {}\".format(dice_val_best, dice_val)\n",
    "        )\n",
    "    else:\n",
    "        print(\n",
    "            \"Model Was Not Saved ! Current Best Avg. Dice: {} Current Avg. Dice: {}\".format(\n",
    "                dice_val_best, dice_val\n",
    "            )\n",
    "        )\n",
    "    # early stopping\n",
    "    if early_stopping.early_stop:\n",
    "        break\n",
    "        \n",
    "model.load_state_dict(torch.load(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd3263a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch_iterator_val = tqdm(val_loader, desc=\"Validate\", dynamic_ncols=True)\n",
    "# for batch in epoch_iterator_val:\n",
    "#  print(1)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e90e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"train completed, best_metric: {dice_val_best:.4f} \" f\"at iteration: {global_step_best}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc083e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_num = 357\n",
    "plt.figure(\"train\", (12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Iteration Average Loss\")\n",
    "x = [eval_num * (i + 1) for i in range(len(epoch_loss_values))]\n",
    "y = epoch_loss_values\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.plot(x, y, label='Train')\n",
    "x = [eval_num * (i + 1) for i in range(len(val_loss_values))]\n",
    "y = val_loss_values\n",
    "plt.plot(x, y, label='Validation')\n",
    "plt.legend()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Val Mean Dice\")\n",
    "x = [eval_num * (i + 1) for i in range(len(metric_values))]\n",
    "y = metric_values\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7a5434",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(model_name))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab8e496",
   "metadata": {},
   "outputs": [],
   "source": [
    "case_num = 4\n",
    "with torch.no_grad():\n",
    "    model.to('cpu')\n",
    "    img_name = os.path.split(val_ds[case_num][\"image\"].meta[\"filename_or_obj\"])[1]\n",
    "    img = val_ds[case_num][\"image\"]\n",
    "    label = val_ds[case_num][\"label\"]\n",
    "    val_inputs = torch.unsqueeze(img, 1).cpu()\n",
    "    val_labels = torch.unsqueeze(label, 1).cpu()\n",
    "    val_outputs = sliding_window_inference(val_inputs, (96, 96, 96), num_samples, model, overlap=0.8)\n",
    "    plt.figure(\"check\", (18, 6))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.title(\"image\")\n",
    "    plt.imshow(val_inputs.cpu().numpy()[0, 0, :, :, 200], cmap=\"gray\")\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.title(\"label\")\n",
    "    plt.imshow(val_labels.cpu().numpy()[0, 0, :, :, 200])\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.title(\"output\")\n",
    "    plt.imshow(torch.argmax(val_outputs, dim=1).detach().cpu()[0, :, :, 200])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1880389c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list_test = generate_file_path(root_path=f'{root}/test')\n",
    "test_ds = PersistentDataset(\n",
    "    data=file_list_test,\n",
    "    transform=val_transforms,\n",
    "    cache_dir='test'\n",
    "#     cache_dir='C:/Training/val'\n",
    ")\n",
    "\n",
    "test_loader = ThreadDataLoader(test_ds, num_workers=0, batch_size=1, \n",
    "                              collate_fn=lambda x: pad_list_data_collate(x, pad_to_shape=(96, 96, 96)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d179da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch_iterator_test):\n",
    "    model.eval()\n",
    "    test_loss = []\n",
    "    with torch.no_grad():\n",
    "        for batch in epoch_iterator_test:\n",
    "            test_inputs, test_labels = (batch[\"image\"].cpu(), batch[\"label\"].cpu())\n",
    "            with torch.cuda.amp.autocast():\n",
    "#                 test_outputs = sliding_window_inference(test_inputs, (96, 96, 96), num_samples, model, overlap=0.8)\n",
    "                test_outputs = sliding_window_inference(test_inputs, (96, 96, 96), num_samples, model)\n",
    "            test_labels_list = decollate_batch(test_labels)\n",
    "            test_labels_convert = [post_label(test_label_tensor) for test_label_tensor in test_labels_list]\n",
    "            test_outputs_list = decollate_batch(test_outputs)\n",
    "            test_output_convert = [post_pred(test_pred_tensor) for test_pred_tensor in test_outputs_list]\n",
    "            loss = loss_function(test_outputs, test_labels)\n",
    "            test_loss.append(loss.item())\n",
    "            dice_metric(y_pred=test_output_convert, y=test_labels_convert)\n",
    "            epoch_iterator_test.set_description(f\"Test (loss={loss.item():2.5f})\")\n",
    "        mean_dice_test = dice_metric.aggregate().item()\n",
    "        dice_metric.reset()\n",
    "\n",
    "        test_loss_mean = np.nanmean(np.nan_to_num(np.array(test_loss),\n",
    "                                               nan=np.nan, posinf=np.nan, neginf=np.nan))\n",
    "    return mean_dice_test, test_loss_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6863d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_iterator_test = tqdm(test_loader, desc=\"Test\", dynamic_ncols=True)\n",
    "dice_test, loss_test = test(epoch_iterator_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37de7a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Test: (loss={loss_test}) (dice={dice_test})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd2b866",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
