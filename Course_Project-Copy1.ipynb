{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONAI version: 1.1.0\n",
      "Numpy version: 1.21.5\n",
      "Pytorch version: 1.13.1+cu117\n",
      "MONAI flags: HAS_EXT = False, USE_COMPILED = False, USE_META_DICT = False\n",
      "MONAI rev id: a2ec3752f54bfc3b40e7952234fbeb5452ed63e3\n",
      "MONAI __file__: /home/emmanuel/PycharmProjects/pythonProject/venv/lib/python3.7/site-packages/monai/__init__.py\n",
      "\n",
      "Optional dependencies:\n",
      "Pytorch Ignite version: 0.4.11\n",
      "Nibabel version: 4.0.2\n",
      "scikit-image version: 0.19.3\n",
      "Pillow version: 9.5.0\n",
      "Tensorboard version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "gdown version: 4.7.1\n",
      "TorchVision version: 0.14.1+cu117\n",
      "tqdm version: 4.65.0\n",
      "lmdb version: 1.4.1\n",
      "psutil version: 5.9.5\n",
      "pandas version: 1.3.5\n",
      "einops version: 0.6.1\n",
      "transformers version: 4.28.1\n",
      "mlflow version: 1.30.1\n",
      "pynrrd version: 1.0.0\n",
      "\n",
      "For details about installing the optional dependencies, please visit:\n",
      "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import concurrent.futures\n",
    "\n",
    "from monai.data.utils import pad_list_data_collate\n",
    "\n",
    "import monai.losses as losses\n",
    "import matplotlib.pyplot as plt\n",
    "from monai.networks.layers import Norm\n",
    "\n",
    "from monai.losses import DiceCELoss\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.transforms import Transform\n",
    "from monai.transforms import (\n",
    "    AsDiscrete,\n",
    "    Compose,\n",
    "    SpatialPad,\n",
    "    SpatialPadd,\n",
    "    CropForegroundd,\n",
    "    LoadImaged,\n",
    "    Orientationd,\n",
    "    RandFlipd,\n",
    "    RandCropByPosNegLabeld,\n",
    "    RandShiftIntensityd,\n",
    "    ScaleIntensityRanged,\n",
    "    NormalizeIntensity,\n",
    "    NormalizeIntensityd,\n",
    "    Spacingd,\n",
    "    RandRotate90d,\n",
    "    EnsureTyped,\n",
    "    RandGaussianNoised,\n",
    ")\n",
    "\n",
    "\n",
    "from monai.config import print_config\n",
    "from monai.metrics import DiceMetric, HausdorffDistanceMetric\n",
    "from monai.networks.nets import UNet\n",
    "\n",
    "from monai.data import (\n",
    "    DataLoader,\n",
    "    ThreadDataLoader,\n",
    "    SmartCacheDataset,\n",
    "    PersistentDataset,\n",
    "    Dataset,\n",
    "    load_decathlon_datalist,\n",
    "    decollate_batch,\n",
    "    set_track_meta,\n",
    ")\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CyclicLR\n",
    "\n",
    "from boundary_loss import BDLoss, DC_and_BD_loss\n",
    "\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model/best_model_dice_ce.pth\n"
     ]
    }
   ],
   "source": [
    "directory = os.environ.get(\"MONAI_DATA_DIRECTORY\")\n",
    "root_dir = 'Model'\n",
    "model_name =  os.path.join(root_dir, \"best_model_dice_ce.pth\")\n",
    "print(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombineBinaryMaps(Transform):\n",
    "    def __init__(self, num_classes, keys):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def __call__(self, data):\n",
    "        binary_maps = data[\"label\"]\n",
    "        combined_map = torch.zeros_like(data['image'][0])\n",
    "#         print(1, data['image'][0].shape, binary_maps[0].shape)\n",
    "\n",
    "        for i in range(self.num_classes):\n",
    "            zero_indices = np.where(combined_map == 0)\n",
    "#             print(zero_indices, binary_maps[i].shape)\n",
    "            combined_map[zero_indices] += (i + 1) * binary_maps[i][zero_indices]\n",
    "#             combined_map %= (self.num_classes + 1)\n",
    "\n",
    "        data[\"label\"] = combined_map.long().unsqueeze(0)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 4\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ToDo: Try windowing with W:350 and L:40\n",
    "# define the window width and level\n",
    "window_width = 350\n",
    "window_level = 40\n",
    "\n",
    "# calculate the intensity range to clip\n",
    "intensity_min = window_level - window_width / 2.0\n",
    "intensity_max = window_level + window_width / 2.0\n",
    "\n",
    "train_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\", \"label\"], ensure_channel_first=True, image_only=False),\n",
    "        CombineBinaryMaps(keys=[\"label\"], num_classes=10),\n",
    "#         RandGaussianNoised(keys=[\"image\"], prob=0.50, mean=0.0, std=0.1),\n",
    "\n",
    "        ScaleIntensityRanged(\n",
    "            keys=[\"image\"],\n",
    "            a_min=-175,\n",
    "            a_max=250,\n",
    "            b_min=0.0,\n",
    "            b_max=1.0,\n",
    "            clip=True,\n",
    "        ),\n",
    "#         CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
    "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "        Spacingd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            pixdim=(1.5, 1.5, 2.0),\n",
    "            mode=(\"bilinear\", \"nearest\"),\n",
    "        ),\n",
    "        SpatialPadd(\n",
    "            spatial_size=(96, 96, 96),\n",
    "            keys=[\"image\", \"label\"]\n",
    "        ),\n",
    "        EnsureTyped(keys=[\"image\", \"label\"], device=device, track_meta=False),\n",
    "       \n",
    "        RandCropByPosNegLabeld(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            label_key=\"label\",\n",
    "            spatial_size=(96, 96, 96),\n",
    "            pos=1,\n",
    "            neg=1,\n",
    "            num_samples=num_samples,\n",
    "            image_key=\"image\",\n",
    "            image_threshold=0,\n",
    "            allow_smaller=True\n",
    "        ),\n",
    "\n",
    "        RandFlipd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            spatial_axis=[0],\n",
    "            prob=0.10,\n",
    "        ),\n",
    "        RandFlipd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            spatial_axis=[1],\n",
    "            prob=0.10,\n",
    "        ),\n",
    "        RandFlipd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            spatial_axis=[2],\n",
    "            prob=0.10,\n",
    "        ),\n",
    "        RandRotate90d(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            prob=0.10,\n",
    "            max_k=3,\n",
    "        ),\n",
    "        RandShiftIntensityd(\n",
    "            keys=[\"image\"],\n",
    "            offsets=0.10,\n",
    "            prob=0.50,\n",
    "        ),\n",
    "\n",
    "    ]\n",
    ")\n",
    "val_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\", \"label\"], ensure_channel_first=True, image_only=False),\n",
    "        CombineBinaryMaps(keys=[\"label\"], num_classes=10),\n",
    "        ScaleIntensityRanged(keys=[\"image\"], a_min=-175, a_max=250, b_min=0.0, b_max=1.0, clip=True),\n",
    "        CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
    "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "        Spacingd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            pixdim=(1.5, 1.5, 2.0),\n",
    "            mode=(\"bilinear\", \"nearest\"),\n",
    "        ),\n",
    "        SpatialPadd(\n",
    "            spatial_size=(96, 96, 96),\n",
    "            keys=[\"image\", \"label\"]\n",
    "        ),\n",
    "        EnsureTyped(keys=[\"image\", \"label\"], device=device, track_meta=True),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_file_path(root_path):\n",
    "    all_files = os.listdir(root_path)\n",
    "    return [{'image': f'{root_path}/{i}/ct.nii.gz',\n",
    "             'label': [f'{root_path}/{i}/segments/{f}' for f in os.listdir(f'{root_path}/{i}/segments')  if not f.startswith(\"._\")],\n",
    "#              'label': f'{root_path}/{i}/labels.nii.gz'\n",
    "\n",
    "            } \n",
    "            for i in all_files if os.path.isfile(f'{root_path}/{i}/ct.nii.gz')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = 'Dataset'\n",
    "# root = 'C:/Training/Dataset'\n",
    "file_list_train = generate_file_path(root_path=f'{root}/train')\n",
    "file_list_val = generate_file_path(root_path=f'{root}/val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [len(x['label']) for x in file_list_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = PersistentDataset(\n",
    "    data=file_list_train,\n",
    "    transform=train_transforms,\n",
    "#     transform=scale,\n",
    "    cache_dir='train_unet'\n",
    ")\n",
    "\n",
    "# train_ds = Dataset(\n",
    "#     data=file_list_train,\n",
    "#     transform=train_transforms,\n",
    "# #     transform=scale,\n",
    "# #     cache_dir='train_unet'\n",
    "# )\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=1, shuffle=True,\n",
    "#                           num_workers=2, pin_memory=False,\n",
    "#                                 collate_fn=collate_fn\n",
    "#                                 collate_fn=lambda x: pad_list_data_collate(x, pad_to_shape=(96, 96, 96))\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds = PersistentDataset(\n",
    "    data=file_list_val,\n",
    "    transform=val_transforms,\n",
    "    cache_dir='val_unet'\n",
    "#     cache_dir='C:/Training/val'\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(val_ds, \n",
    "#                         num_workers=0,\n",
    "                        batch_size=1, pin_memory=False,\n",
    "#                               collate_fn=lambda x: pad_list_data_collate(x, pad_to_shape=(96, 96, 96))\n",
    "                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, tolerance=5, min_delta=0):\n",
    "\n",
    "        self.tolerance = tolerance\n",
    "        self.min_delta = min_delta\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = loss\n",
    "        else:\n",
    "            if (self.best_loss - loss) > self.min_delta:\n",
    "                self.best_loss = loss\n",
    "                self.counter = 0\n",
    "            else:\n",
    "                self.counter += 1\n",
    "                print(f'Early Stopping patience: {self.counter}, best loss: {self.best_loss}, current_loss: {loss}')\n",
    "                if self.counter >= self.tolerance:  \n",
    "                    self.early_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available GPUs: 1\n",
      "GPU 0: GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device_count = torch.cuda.device_count()\n",
    "    print(f\"Number of available GPUs: {device_count}\")\n",
    "    for i in range(device_count):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    print(\"No GPU found, using CPU instead.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = UNet(\n",
    "#               spatial_dims=3,\n",
    "#               in_channels=1,\n",
    "#               out_channels=11,\n",
    "#               channels=(64, 128, 256),\n",
    "#               strides=(2, 2),\n",
    "#               num_res_units=2,\n",
    "#               norm=Norm.BATCH\n",
    "#                 ).to(device)\n",
    "\n",
    "model = UNet(\n",
    "    spatial_dims=3,\n",
    "    in_channels=1,\n",
    "    out_channels=11,\n",
    "    channels=(16, 32, 64, 128, 256),\n",
    "    strides=(2, 2, 2, 2),\n",
    "    num_res_units=2,\n",
    "    norm=Norm.BATCH,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "early_stopping = EarlyStopping(tolerance=10, min_delta=0.001)\n",
    "loss_function = DiceCELoss(to_onehot_y=True, softmax=True, lambda_dice=1, lambda_ce=1)\n",
    "bd_loss =  BDLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "scheduler = ReduceLROnPlateau(optimizer, patience=10, verbose=True, mode='min', min_lr=1e-8, factor=0.9)\n",
    "scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = SpatialPad(\n",
    "            (1, 96, 96, 96),\n",
    "        )\n",
    "def validation(epoch_iterator_val):\n",
    "    model.eval()\n",
    "    validation_loss = []\n",
    "    with torch.no_grad():\n",
    "        for batch in epoch_iterator_val:\n",
    "            val_inputs, val_labels = (batch[\"image\"].cuda(), batch[\"label\"].cuda())\n",
    "            with torch.cuda.amp.autocast():\n",
    "                val_outputs = sliding_window_inference(val_inputs, (96, 96, 96), num_samples, model)\n",
    "            val_labels_list = decollate_batch(val_labels)\n",
    "            val_labels_convert = [post_label(val_label_tensor) for val_label_tensor in val_labels_list]\n",
    "            val_outputs_list = decollate_batch(val_outputs)\n",
    "            val_output_convert = [post_pred(val_pred_tensor) for val_pred_tensor in val_outputs_list]\n",
    "            loss = loss_function(val_outputs, val_labels)\n",
    "            validation_loss.append(loss.item())\n",
    "            dice_metric(y_pred=val_output_convert, y=val_labels_convert)\n",
    "            epoch_iterator_val.set_description(f\"Validate (loss={loss.item():2.5f})\")\n",
    "        mean_dice_val = dice_metric.aggregate().item()\n",
    "        dice_metric.reset()\n",
    "        \n",
    "        mean_hausdorff = 0\n",
    "\n",
    "        validation_loss_mean = np.nanmean(np.nan_to_num(np.array(validation_loss),\n",
    "                                               nan=np.nan, posinf=np.nan, neginf=np.nan))\n",
    "\n",
    "        val_loss_values.append(validation_loss_mean)\n",
    "    return mean_dice_val, mean_hausdorff, validation_loss_mean\n",
    "\n",
    "\n",
    "def train(global_step, train_loader, dice_val_best, global_step_best):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    step = 0\n",
    "    epoch_iterator = tqdm(train_loader, desc=\"Training (X / X Steps) (loss=X.X)\", dynamic_ncols=True)\n",
    "    for step, batch in enumerate(epoch_iterator):\n",
    "        step += 1\n",
    "#         print(step)\n",
    "        x, y = (batch[\"image\"].cuda(), batch[\"label\"].cuda())\n",
    "#         a = all(t.shape == x.shape for t in y)\n",
    "        if x.shape != y.shape:\n",
    "            print(x.shape, y.shape)\n",
    "#         with torch.cuda.amp.autocast():\n",
    "#             logit_map = model(x)\n",
    "#             loss = loss_function(logit_map, y)\n",
    "#         scaler.scale(loss).backward()\n",
    "#         epoch_loss += loss.item()\n",
    "#         if torch.isnan(loss):\n",
    "#             print(torch.max(x), torch.min(x))\n",
    "            \n",
    "#         scaler.unscale_(optimizer)\n",
    "\n",
    "#         scaler.step(optimizer)\n",
    "\n",
    "#         scaler.update()\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         epoch_iterator.set_description(f\"Training ({global_step} / {max_iterations} Steps) (loss={loss.item():2.5f})\")\n",
    "#         global_step += 1\n",
    "       \n",
    "\n",
    "    epoch_loss /= step\n",
    "    return global_step, dice_val_best, global_step_best, epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (X / X Steps) (loss=X.X):   1%|▊                                                                                                                                                  | 2/357 [00:00<01:03,  5.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1, 96, 96, 96]) torch.Size([4, 10, 96, 96, 96])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (X / X Steps) (loss=X.X):   1%|██                                                                                                                                                 | 5/357 [00:01<02:36,  2.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1, 96, 96, 96]) torch.Size([4, 10, 96, 96, 96])\n",
      "1 torch.Size([236, 236, 428]) torch.Size([236, 236, 428])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (X / X Steps) (loss=X.X):   2%|██▉                                                                                                                                                | 7/357 [00:08<09:38,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1, 96, 96, 96]) torch.Size([4, 10, 96, 96, 96])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training (X / X Steps) (loss=X.X):   2%|███▎                                                                                                                                               | 8/357 [00:08<07:13,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1, 96, 96, 96]) torch.Size([4, 10, 96, 96, 96])\n",
      "1 torch.Size([285, 285, 427]) torch.Size([285, 285, 427])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training (X / X Steps) (loss=X.X):   3%|███▋                                                                                                                                               | 9/357 [00:17<19:48,  3.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 torch.Size([264, 264, 419]) torch.Size([264, 264, 419])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training (X / X Steps) (loss=X.X):   3%|████                                                                                                                                              | 10/357 [00:24<26:22,  4.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 torch.Size([51, 153, 393]) torch.Size([51, 153, 393])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (X / X Steps) (loss=X.X):   3%|████▉                                                                                                                                             | 12/357 [00:25<14:27,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1, 96, 96, 96]) torch.Size([4, 10, 96, 96, 96])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training (X / X Steps) (loss=X.X):   4%|█████▎                                                                                                                                            | 13/357 [00:26<11:28,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1, 96, 96, 96]) torch.Size([4, 10, 96, 96, 96])\n",
      "1 torch.Size([145, 145, 147]) torch.Size([145, 145, 147])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training (X / X Steps) (loss=X.X):   4%|█████▋                                                                                                                                            | 14/357 [00:27<09:13,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 torch.Size([212, 212, 205]) torch.Size([212, 212, 205])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training (X / X Steps) (loss=X.X):   4%|██████▏                                                                                                                                           | 15/357 [00:29<10:13,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 torch.Size([281, 281, 409]) torch.Size([281, 281, 409])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (X / X Steps) (loss=X.X):   5%|██████▉                                                                                                                                           | 17/357 [00:38<16:21,  2.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1, 96, 96, 96]) torch.Size([4, 10, 96, 96, 96])\n",
      "1 torch.Size([195, 195, 270]) torch.Size([195, 195, 270])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (X / X Steps) (loss=X.X):   5%|███████▎                                                                                                                                          | 18/357 [00:42<13:20,  2.36s/it]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "applying transform <monai.transforms.croppad.dictionary.RandCropByPosNegLabeld object at 0x7f09cf799ad0>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m~/PycharmProjects/pythonProject/venv/lib/python3.7/site-packages/monai/transforms/transform.py\u001b[0m in \u001b[0;36mapply_transform\u001b[0;34m(transform, data, map_items, unpack_items, log_stats)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_apply_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munpack_items\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_apply_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munpack_items\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/pythonProject/venv/lib/python3.7/site-packages/monai/transforms/transform.py\u001b[0m in \u001b[0;36m_apply_transform\u001b[0;34m(transform, parameters, unpack_parameters)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/pythonProject/venv/lib/python3.7/site-packages/monai/transforms/croppad/dictionary.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    860\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 861\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandomize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfg_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbg_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    862\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/pythonProject/venv/lib/python3.7/site-packages/monai/transforms/croppad/dictionary.py\u001b[0m in \u001b[0;36mrandomize\u001b[0;34m(self, label, fg_indices, bg_indices, image)\u001b[0m\n\u001b[1;32m    851\u001b[0m     ) -> None:\n\u001b[0;32m--> 852\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcropper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandomize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfg_indices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfg_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbg_indices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbg_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/pythonProject/venv/lib/python3.7/site-packages/monai/transforms/croppad/array.py\u001b[0m in \u001b[0;36mrandomize\u001b[0;34m(self, label, fg_indices, bg_indices, image)\u001b[0m\n\u001b[1;32m   1059\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1060\u001b[0;31m                 \u001b[0mfg_indices_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbg_indices_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap_binary_to_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_threshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1061\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/pythonProject/venv/lib/python3.7/site-packages/monai/transforms/utils.py\u001b[0m in \u001b[0;36mmap_binary_to_indices\u001b[0;34m(label, image, image_threshold)\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0mimg_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_dst_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimg_flat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m         \u001b[0mbg_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_flat\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m~\u001b[0m\u001b[0mlabel_flat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/pythonProject/venv/lib/python3.7/site-packages/monai/transforms/utils_pytorch_numpy_unification.py\u001b[0m in \u001b[0;36mnonzero\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 124.00 MiB (GPU 0; 10.76 GiB total capacity; 2.20 GiB already allocated; 126.19 MiB free; 2.42 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3333/2837932286.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mval_loss_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mglobal_step\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmax_iterations\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdice_val_best\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step_best\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdice_val_best\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step_best\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mepoch_iterator_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Validate\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdynamic_ncols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mdice_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhausdorff_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_iterator_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3333/1299014987.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(global_step, train_loader, dice_val_best, global_step_best)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mepoch_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Training (X / X Steps) (loss=X.X)\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdynamic_ncols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mstep\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m#         print(step)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/pythonProject/venv/lib/python3.7/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1179\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/pythonProject/venv/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    626\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    629\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/pythonProject/venv/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/pythonProject/venv/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/pythonProject/venv/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/pythonProject/venv/lib/python3.7/site-packages/monai/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;31m# dataset[[1, 3, 4]]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mSubset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/pythonProject/venv/lib/python3.7/site-packages/monai/data/dataset.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    409\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m         \u001b[0mpre_random_item\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cachecheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_post_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpre_random_item\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/pythonProject/venv/lib/python3.7/site-packages/monai/data/dataset.py\u001b[0m in \u001b[0;36m_post_transform\u001b[0;34m(self, item_transformed)\u001b[0m\n\u001b[1;32m    346\u001b[0m             ):\n\u001b[1;32m    347\u001b[0m                 \u001b[0mstart_post_randomize_run\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m                 \u001b[0mitem_transformed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem_transformed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mitem_transformed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/pythonProject/venv/lib/python3.7/site-packages/monai/transforms/transform.py\u001b[0m in \u001b[0;36mapply_transform\u001b[0;34m(transform, data, map_items, unpack_items, log_stats)\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m                 \u001b[0m_log_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"applying transform {transform}\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: applying transform <monai.transforms.croppad.dictionary.RandCropByPosNegLabeld object at 0x7f09cf799ad0>"
     ]
    }
   ],
   "source": [
    "max_iterations = 90000\n",
    "post_label = AsDiscrete(to_onehot=11)\n",
    "post_pred = AsDiscrete(argmax=True, to_onehot=11)\n",
    "dice_metric = DiceMetric(include_background=True, reduction=\"mean\", get_not_nans=False)\n",
    "hausdorff_metric = HausdorffDistanceMetric(include_background=True, get_not_nans=False, reduction=\"none\",\n",
    "                                           distance_metric=\"euclidean\")\n",
    "global_step = 0\n",
    "dice_val_best = 0.0\n",
    "global_step_best = 0\n",
    "epoch_loss_values = []\n",
    "metric_values = []\n",
    "val_loss_values = []\n",
    "while global_step < max_iterations:\n",
    "    global_step, dice_val_best, global_step_best, epoch_loss = train(global_step, train_loader, dice_val_best, global_step_best)\n",
    "    epoch_iterator_val = tqdm(val_loader, desc=\"Validate\", dynamic_ncols=True)\n",
    "    dice_val, hausdorff_val, loss_val = validation(epoch_iterator_val)\n",
    "    scheduler.step(epoch_loss)\n",
    "    epoch_loss_values.append(epoch_loss)\n",
    "    metric_values.append(dice_val)\n",
    "    early_stopping(loss_val)\n",
    "    print(f'Mean Hausdorff disatnce: {hausdorff_val}')\n",
    "    if dice_val > dice_val_best:\n",
    "        dice_val_best = dice_val\n",
    "        global_step_best = global_step\n",
    "        torch.save(model.state_dict(), model_name)\n",
    "        print(\n",
    "            \"Model Was Saved ! Current Best Avg. Dice: {} Current Avg. Dice: {}\".format(dice_val_best, dice_val)\n",
    "        )\n",
    "    else:\n",
    "        print(\n",
    "            \"Model Was Not Saved ! Current Best Avg. Dice: {} Current Avg. Dice: {}\".format(\n",
    "                dice_val_best, dice_val\n",
    "            )\n",
    "        )\n",
    "    # early stopping\n",
    "    if early_stopping.early_stop:\n",
    "        break\n",
    "model.load_state_dict(torch.load(model_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"train completed, best_metric: {dice_val_best:.4f} \" f\"at iteration: {global_step_best}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(\"train\", (12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Iteration Average Loss\")\n",
    "x = [eval_num * (i + 1) for i in range(len(epoch_loss_values))]\n",
    "y = epoch_loss_values\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.plot(x, y)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Val Mean Dice\")\n",
    "x = [eval_num * (i + 1) for i in range(len(metric_values))]\n",
    "y = metric_values\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(model_name))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_num = 4\n",
    "with torch.no_grad():\n",
    "    img_name = os.path.split(val_ds[case_num][\"image\"].meta[\"filename_or_obj\"])[1]\n",
    "    img = val_ds[case_num][\"image\"]\n",
    "    label = val_ds[case_num][\"label\"]\n",
    "    val_inputs = torch.unsqueeze(img, 1).to(device)\n",
    "    val_labels = torch.unsqueeze(label, 1).to(device)\n",
    "    val_outputs = sliding_window_inference(val_inputs, (96, 96, 96), num_samples, model, overlap=0.8)\n",
    "    plt.figure(\"check\", (18, 6))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.title(\"image\")\n",
    "    plt.imshow(val_inputs.cpu().numpy()[0, 0, :, :, 200], cmap=\"gray\")\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.title(\"label\")\n",
    "    plt.imshow(val_labels.cpu().numpy()[0, 0, :, :, 200])\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.title(\"output\")\n",
    "    plt.imshow(torch.argmax(val_outputs, dim=1).detach().cpu()[0, :, :, 200])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list_test = generate_file_path(root_path=f'{root}/test')\n",
    "test_ds = PersistentDataset(\n",
    "    data=file_list_test,\n",
    "    transform=val_transforms,\n",
    "    cache_dir='test'\n",
    "#     cache_dir='C:/Training/val'\n",
    ")\n",
    "\n",
    "test_loader = ThreadDataLoader(test_ds, num_workers=0, batch_size=1, \n",
    "                              collate_fn=lambda x: pad_list_data_collate(x, pad_to_shape=(96, 96, 96)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch_iterator_test):\n",
    "    model.eval()\n",
    "    test_loss = []\n",
    "    with torch.no_grad():\n",
    "        for batch in epoch_iterator_test:\n",
    "            test_inputs, test_labels = (batch[\"image\"].to(device), batch[\"label\"].to(device))\n",
    "            with torch.cuda.amp.autocast():\n",
    "#                 test_outputs = sliding_window_inference(test_inputs, (96, 96, 96), num_samples, model, overlap=0.8)\n",
    "                test_outputs = sliding_window_inference(test_inputs, (96, 96, 96), num_samples, model)\n",
    "            test_labels_list = decollate_batch(test_labels)\n",
    "            test_labels_convert = [post_label(test_label_tensor) for test_label_tensor in test_labels_list]\n",
    "            test_outputs_list = decollate_batch(test_outputs)\n",
    "            test_output_convert = [post_pred(test_pred_tensor) for test_pred_tensor in test_outputs_list]\n",
    "            loss = loss_function(test_outputs, test_labels)\n",
    "            test_loss.append(loss.item())\n",
    "            dice_metric(y_pred=test_output_convert, y=test_labels_convert)\n",
    "            epoch_iterator_test.set_description(f\"Test (loss={loss.item():2.5f})\")\n",
    "        mean_dice_test = dice_metric.aggregate().item()\n",
    "        dice_metric.reset()\n",
    "\n",
    "        test_loss_mean = np.nanmean(np.nan_to_num(np.array(test_loss),\n",
    "                                               nan=np.nan, posinf=np.nan, neginf=np.nan))\n",
    "    return mean_dice_test, test_loss_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_iterator_test = tqdm(test_loader, desc=\"Test\", dynamic_ncols=True)\n",
    "dice_test, loss_test = test(epoch_iterator_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Test: (loss={loss_test}) (dice={dice_test})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
